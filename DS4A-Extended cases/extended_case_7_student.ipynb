{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies to prevent overfitting in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "try:\n",
    "    tf.set_random_seed(1337)                    # set the random seed for reproducibility\n",
    "except:\n",
    "    tf.random.set_seed(1337)                     # NOTE: Newer version of tensorflow uses tf.random.set_seed\n",
    "np.random.seed(1337)                         #       instead of tf.set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Business Context.** You are a data scientist working for a machine learning consultancy. One of your clients wants to be able to classify text reviews automatically by the likely rating (on a 1 - 5 scale) that that person would give. However, they do not have sufficient data they generated on their own to do this, so you need to use an external, rich dataset as a basis on which to build your model and then translate it over.\n",
    "\n",
    "**Business Problem.** Your task is to **build a neural networks-based model for classifying text reviews into likely ratings (on a 1 - 5 scale)**.\n",
    "\n",
    "**Analytical Context.** We'll use the Amazon review dataset again and try to classify reviews into star ratings automatically. Instead of just positive and negative, we'll take on the harder challenge of predicting the *exact* star rating. The lowest score is 1 and the highest is 5.\n",
    "\n",
    "Instead of trying to optimize by pre-processing the text, we'll do very basic tokenization and experiment with different neural network models, architectures, and hyperparameters to optimize the results. You'll start by building a simple dense neural network and try to get it to perform better using various techniques. Then you'll evaluate the results and diagnose where it tends to perform more poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and preparing the data\n",
    "\n",
    "We'll mainly be using the `keras` module from TensorFlow, but we'll also use `pandas` to read the CSV file and `sklearn` for some helper functions. We'll be using only the \"Text\" and \"Score\" columns in the `Reviews.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews = pd.read_csv('Reviews.csv', nrows=262084)\n",
    "amazon_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Combine the first 1,000 of each of the 1-, 2-, 3-, 4-, and 5-star reviews in `amazon_reviews` into a single DataFrame (so you should have 5,000 observations in total). Split this DataFrame into training and test sets, with 80% of the data for the training set.\n",
    "\n",
    "**Hint:** `keras` will expected your labels to start with 0, and not 1, so make sure to adjust the labels accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\envs\\extended_case_7\\lib\\site-packages\\pandas\\core\\generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "amazon_rev_sorted = amazon_reviews.sort_values(by=\"Score\")[[\"Score\",\"Text\"]]\n",
    "\n",
    "amazon_rev_reduced = amazon_rev_sorted.groupby([\"Score\"]).head(1000)\n",
    "amazon_rev_reduced.Score = amazon_rev_reduced.Score.apply(lambda x: x-1)\n",
    "# amazon_rev_reduced[amazon_rev_reduced.Score == 0]\n",
    "\n",
    "rev_train, rev_test = train_test_split(amazon_rev_reduced, train_size=0.8, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing our texts\n",
    "\n",
    "Keras comes with its own functions to preprocess text, including a [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) (a mapping from each word in our corpus to a unique integer). Unlike the `CountVectorizer` from `sklearn`, which produces sparse matrices, `keras` often expects to work with sequences representing only the words that occur in a text. To prepare text before feeding it into a neural network, we usually:\n",
    "\n",
    "1. Create a [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer).\n",
    "2. [Create sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences) from our text (each text becomes a list of integers, based on the tokenizer mapping, instead of words)\n",
    "3. [Pad or truncate](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) each sequence to a fixed length (very short texts get `0`s added to them, while very long ones are truncated).\n",
    "\n",
    "The tokenizer has a configurable word cap, so it will only consider the $n$ most common words in the corpus, ignoring very rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "In this exercise, you will learn how to use the `tf.keras.preprocessing.text.Tokenizer` tool to carry out the preprocessing steps described above.\n",
    "\n",
    "#### 2.1\n",
    "\n",
    "Perform some exploratory analysis of the dataset to calculate the number of unique words in our corpus and the distribution of the number of words in each review of the training set. What is the 80th percentile of this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\envs\\extended_case_7\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37138 different words in the original corpus\n",
      "The 80th percentile is:  29710\n",
      "There are 14962 different words in the corpus after using only lowercase and removing special characters\n",
      "The 80th percentile is: 11969\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "amazon_rev_reduced[\"text_new\"] = amazon_rev_reduced.Text.apply(lambda x: x.lower())\n",
    "\n",
    "special_chars = amazon_rev_reduced.text_new.apply(lambda x: [each for each in list(x) if not each.isalnum() and each != ' '])\n",
    "flat_list = [item for sublist in special_chars for item in sublist]\n",
    "# print(set(flat_list))\n",
    "\n",
    "amazon_rev_reduced_orig = amazon_rev_reduced.copy()\n",
    "\n",
    "amazon_rev_reduced.text_new = amazon_rev_reduced.text_new.apply(\n",
    "    lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "\n",
    "words_orig = amazon_rev_reduced.Text.apply(lambda x: x.split(\" \"))\n",
    "words_orig = [word for sublist in words_orig for word in sublist]\n",
    "\n",
    "words_prep = amazon_rev_reduced.text_new.apply(lambda x: x.split(\" \"))\n",
    "words_prep = [word.strip() for words in words_prep for word in words]\n",
    "\n",
    "print(f\"There are {len(set(words_orig))} different words in the original corpus\")\n",
    "print(f\"The 80th percentile is: \", int(len(set(words_orig)) * 0.8))\n",
    "\n",
    "print(f\"There are {len(set(words_prep))} different words in the corpus after using only lowercase and removing special characters\")\n",
    "print(f\"The 80th percentile is: {int(len(set(words_prep)) * 0.8)}\")\n",
    "\n",
    "# amazon_rev_reduced.head()\n",
    "# amazon_rev_reduced.text_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of number of words per review in the train dataset')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "words_per_review = rev_train.Text.apply(lambda x: len(x.split(\" \")))\n",
    "words_per_review.hist(bins = 100)\n",
    "\n",
    "plt.title(\"Histogram of number of words per review in the train dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4000.000000\n",
       "mean       93.326000\n",
       "std        98.810504\n",
       "min         7.000000\n",
       "25%        38.000000\n",
       "50%        65.000000\n",
       "75%       112.000000\n",
       "max      1387.000000\n",
       "Name: Text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_review.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.326"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_review.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the train dataset, the 80th quantile of the number of words per review is 127.0\n"
     ]
    }
   ],
   "source": [
    "print(\"In the train dataset, the 80th quantile of the number of words per review is\",words_per_review.quantile(0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2\n",
    "\n",
    "Given the results above, we create a tokenizer using only the top 20,000 most frequent words in our corpus (which corresponds to roughly 80% of the words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000) #We create the tokenizer using only top 20000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(rev_train['Text'])  #Then, we create the text->indices mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line has given several features and methods to our tokenizer. For instance, print the line `tokenizer.word_index` in a new cell - what do you see? Apply the `tokenizer.texts_to_sequences()` method on the list `['I just feel very very good']`. Apply the `tokenizer.sequences_to_texts()` method on the list `[[109, 19, 824, 76, 114, 6315, 1137, 8070]]`. What were your results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'a': 3,\n",
       " 'and': 4,\n",
       " 'to': 5,\n",
       " 'it': 6,\n",
       " 'of': 7,\n",
       " 'br': 8,\n",
       " 'is': 9,\n",
       " 'this': 10,\n",
       " 'in': 11,\n",
       " 'for': 12,\n",
       " 'that': 13,\n",
       " 'my': 14,\n",
       " 'but': 15,\n",
       " 'not': 16,\n",
       " 'was': 17,\n",
       " 'with': 18,\n",
       " 'have': 19,\n",
       " 'you': 20,\n",
       " 'as': 21,\n",
       " 'like': 22,\n",
       " 'are': 23,\n",
       " 'they': 24,\n",
       " 'so': 25,\n",
       " 'on': 26,\n",
       " 'be': 27,\n",
       " 'these': 28,\n",
       " 'good': 29,\n",
       " 'taste': 30,\n",
       " 'them': 31,\n",
       " 'if': 32,\n",
       " 'product': 33,\n",
       " 'just': 34,\n",
       " 'coffee': 35,\n",
       " 'or': 36,\n",
       " 'one': 37,\n",
       " 'at': 38,\n",
       " 'more': 39,\n",
       " 'very': 40,\n",
       " 'would': 41,\n",
       " 'all': 42,\n",
       " 'from': 43,\n",
       " 'food': 44,\n",
       " 'out': 45,\n",
       " 'flavor': 46,\n",
       " 'had': 47,\n",
       " 'me': 48,\n",
       " \"it's\": 49,\n",
       " 'can': 50,\n",
       " 'when': 51,\n",
       " 'will': 52,\n",
       " 'has': 53,\n",
       " 'get': 54,\n",
       " 'no': 55,\n",
       " 'really': 56,\n",
       " 'than': 57,\n",
       " 'we': 58,\n",
       " 'because': 59,\n",
       " 'other': 60,\n",
       " 'she': 61,\n",
       " 'about': 62,\n",
       " 'great': 63,\n",
       " 'up': 64,\n",
       " 'some': 65,\n",
       " 'much': 66,\n",
       " \"don't\": 67,\n",
       " 'only': 68,\n",
       " 'too': 69,\n",
       " 'were': 70,\n",
       " 'what': 71,\n",
       " 'an': 72,\n",
       " 'there': 73,\n",
       " 'your': 74,\n",
       " 'tea': 75,\n",
       " 'little': 76,\n",
       " 'even': 77,\n",
       " 'do': 78,\n",
       " 'which': 79,\n",
       " 'amazon': 80,\n",
       " 'by': 81,\n",
       " 'cup': 82,\n",
       " 'love': 83,\n",
       " 'also': 84,\n",
       " 'after': 85,\n",
       " \"i'm\": 86,\n",
       " 'buy': 87,\n",
       " 'her': 88,\n",
       " 'tried': 89,\n",
       " 'eat': 90,\n",
       " 'time': 91,\n",
       " 'he': 92,\n",
       " 'better': 93,\n",
       " 'am': 94,\n",
       " 'been': 95,\n",
       " 'price': 96,\n",
       " 'use': 97,\n",
       " 'then': 98,\n",
       " 'any': 99,\n",
       " 'first': 100,\n",
       " 'now': 101,\n",
       " 'try': 102,\n",
       " 'again': 103,\n",
       " 'well': 104,\n",
       " 'chocolate': 105,\n",
       " 'find': 106,\n",
       " 'dog': 107,\n",
       " \"i've\": 108,\n",
       " 'make': 109,\n",
       " 'their': 110,\n",
       " 'box': 111,\n",
       " 'did': 112,\n",
       " 'sugar': 113,\n",
       " 'think': 114,\n",
       " '2': 115,\n",
       " 'way': 116,\n",
       " 'made': 117,\n",
       " '3': 118,\n",
       " 'still': 119,\n",
       " 'bag': 120,\n",
       " '1': 121,\n",
       " 'water': 122,\n",
       " 'cat': 123,\n",
       " 'used': 124,\n",
       " 'got': 125,\n",
       " 'something': 126,\n",
       " \"didn't\": 127,\n",
       " 'know': 128,\n",
       " 'bought': 129,\n",
       " 'day': 130,\n",
       " 'free': 131,\n",
       " 'best': 132,\n",
       " 'sweet': 133,\n",
       " 'could': 134,\n",
       " 'bad': 135,\n",
       " 'drink': 136,\n",
       " 'over': 137,\n",
       " 'two': 138,\n",
       " 'our': 139,\n",
       " 'does': 140,\n",
       " 'since': 141,\n",
       " 'thought': 142,\n",
       " 'however': 143,\n",
       " '4': 144,\n",
       " 'ingredients': 145,\n",
       " 'who': 146,\n",
       " 'order': 147,\n",
       " 'back': 148,\n",
       " 'bit': 149,\n",
       " 'found': 150,\n",
       " '5': 151,\n",
       " 'tastes': 152,\n",
       " 'off': 153,\n",
       " 'same': 154,\n",
       " 'less': 155,\n",
       " 'eating': 156,\n",
       " 'few': 157,\n",
       " 'most': 158,\n",
       " 'give': 159,\n",
       " 'want': 160,\n",
       " 'products': 161,\n",
       " 'into': 162,\n",
       " 'before': 163,\n",
       " 'mix': 164,\n",
       " 'never': 165,\n",
       " 'though': 166,\n",
       " \"doesn't\": 167,\n",
       " 'k': 168,\n",
       " 'many': 169,\n",
       " 'store': 170,\n",
       " 'go': 171,\n",
       " 'different': 172,\n",
       " 'while': 173,\n",
       " 'diet': 174,\n",
       " 'how': 175,\n",
       " 'flavors': 176,\n",
       " 'real': 177,\n",
       " 'enough': 178,\n",
       " 'cats': 179,\n",
       " \"can't\": 180,\n",
       " 'gluten': 181,\n",
       " 'should': 182,\n",
       " 'brand': 183,\n",
       " 'nice': 184,\n",
       " 'going': 185,\n",
       " 'thing': 186,\n",
       " 'sure': 187,\n",
       " 'looking': 188,\n",
       " 'regular': 189,\n",
       " 'around': 190,\n",
       " 'its': 191,\n",
       " 'pretty': 192,\n",
       " 'stuff': 193,\n",
       " 'always': 194,\n",
       " 'being': 195,\n",
       " 'milk': 196,\n",
       " 'using': 197,\n",
       " 'small': 198,\n",
       " 'work': 199,\n",
       " 'down': 200,\n",
       " 'hot': 201,\n",
       " 'cups': 202,\n",
       " 'ordered': 203,\n",
       " 'lot': 204,\n",
       " 'say': 205,\n",
       " 'strong': 206,\n",
       " 'every': 207,\n",
       " 'package': 208,\n",
       " 'see': 209,\n",
       " 'ever': 210,\n",
       " 'old': 211,\n",
       " 'kind': 212,\n",
       " 'put': 213,\n",
       " 'treats': 214,\n",
       " 'high': 215,\n",
       " 'hard': 216,\n",
       " 'three': 217,\n",
       " 'may': 218,\n",
       " 'him': 219,\n",
       " 'pack': 220,\n",
       " 'years': 221,\n",
       " 'bags': 222,\n",
       " 'long': 223,\n",
       " 'quality': 224,\n",
       " 'trying': 225,\n",
       " 'recommend': 226,\n",
       " 'whole': 227,\n",
       " 'quite': 228,\n",
       " 'dogs': 229,\n",
       " 'right': 230,\n",
       " 'juice': 231,\n",
       " 'tasted': 232,\n",
       " 'money': 233,\n",
       " 'buying': 234,\n",
       " 'bottle': 235,\n",
       " 'natural': 236,\n",
       " 'another': 237,\n",
       " 'those': 238,\n",
       " 'his': 239,\n",
       " 'makes': 240,\n",
       " 'size': 241,\n",
       " 'without': 242,\n",
       " 'chicken': 243,\n",
       " 'away': 244,\n",
       " 'probably': 245,\n",
       " 'new': 246,\n",
       " 'healthy': 247,\n",
       " 'treat': 248,\n",
       " '6': 249,\n",
       " 'smell': 250,\n",
       " 'maybe': 251,\n",
       " 'received': 252,\n",
       " 'bars': 253,\n",
       " 'far': 254,\n",
       " 'might': 255,\n",
       " 'snack': 256,\n",
       " 'amount': 257,\n",
       " 'keep': 258,\n",
       " 'things': 259,\n",
       " 'disappointed': 260,\n",
       " 'people': 261,\n",
       " 'need': 262,\n",
       " 'salt': 263,\n",
       " 'actually': 264,\n",
       " 'each': 265,\n",
       " 'take': 266,\n",
       " 'shipping': 267,\n",
       " 'feel': 268,\n",
       " 'here': 269,\n",
       " 'why': 270,\n",
       " \"i'd\": 271,\n",
       " 'favorite': 272,\n",
       " 'big': 273,\n",
       " 'fresh': 274,\n",
       " 'through': 275,\n",
       " 'half': 276,\n",
       " 'organic': 277,\n",
       " 'said': 278,\n",
       " 'item': 279,\n",
       " '10': 280,\n",
       " 'almost': 281,\n",
       " 'dry': 282,\n",
       " 'per': 283,\n",
       " 'stars': 284,\n",
       " 'both': 285,\n",
       " 'candy': 286,\n",
       " 'cookies': 287,\n",
       " 'green': 288,\n",
       " '8': 289,\n",
       " 'oz': 290,\n",
       " 'definitely': 291,\n",
       " 'add': 292,\n",
       " \"isn't\": 293,\n",
       " 'company': 294,\n",
       " 'anything': 295,\n",
       " 'year': 296,\n",
       " 'enjoy': 297,\n",
       " 'expensive': 298,\n",
       " 'texture': 299,\n",
       " 'problem': 300,\n",
       " 'butter': 301,\n",
       " 'fine': 302,\n",
       " \"you're\": 303,\n",
       " 'worth': 304,\n",
       " 'purchased': 305,\n",
       " 'foods': 306,\n",
       " 'tasting': 307,\n",
       " 'review': 308,\n",
       " 'calories': 309,\n",
       " \"that's\": 310,\n",
       " 'own': 311,\n",
       " 'nothing': 312,\n",
       " 'plastic': 313,\n",
       " 'delicious': 314,\n",
       " 'instead': 315,\n",
       " 'pretzels': 316,\n",
       " 'getting': 317,\n",
       " 'cereal': 318,\n",
       " 'look': 319,\n",
       " 'least': 320,\n",
       " 'easy': 321,\n",
       " 'chips': 322,\n",
       " 'rice': 323,\n",
       " 'reviews': 324,\n",
       " 'come': 325,\n",
       " \"wasn't\": 326,\n",
       " 'happy': 327,\n",
       " \"won't\": 328,\n",
       " 'loves': 329,\n",
       " 'corn': 330,\n",
       " 'purchase': 331,\n",
       " 'fruit': 332,\n",
       " 'tasty': 333,\n",
       " 'local': 334,\n",
       " 'packaging': 335,\n",
       " 'months': 336,\n",
       " 'toy': 337,\n",
       " 'times': 338,\n",
       " 'flavored': 339,\n",
       " 'rather': 340,\n",
       " 'once': 341,\n",
       " 'morning': 342,\n",
       " 'low': 343,\n",
       " 'came': 344,\n",
       " 'mouth': 345,\n",
       " 'took': 346,\n",
       " 'open': 347,\n",
       " 'last': 348,\n",
       " '12': 349,\n",
       " 'large': 350,\n",
       " 'gave': 351,\n",
       " 'vanilla': 352,\n",
       " 'case': 353,\n",
       " 'sauce': 354,\n",
       " 'full': 355,\n",
       " 'able': 356,\n",
       " 'wanted': 357,\n",
       " 'com': 358,\n",
       " 'oil': 359,\n",
       " 'bar': 360,\n",
       " 'energy': 361,\n",
       " 'until': 362,\n",
       " 'protein': 363,\n",
       " 'such': 364,\n",
       " 'usually': 365,\n",
       " 'seems': 366,\n",
       " 'cholesterol': 367,\n",
       " 'having': 368,\n",
       " 'several': 369,\n",
       " 'days': 370,\n",
       " 'house': 371,\n",
       " \"i'll\": 372,\n",
       " 'save': 373,\n",
       " 'bitter': 374,\n",
       " 'black': 375,\n",
       " 'science': 376,\n",
       " 'comes': 377,\n",
       " 'loved': 378,\n",
       " 'powder': 379,\n",
       " 'seem': 380,\n",
       " 'hair': 381,\n",
       " 'top': 382,\n",
       " 'coconut': 383,\n",
       " 'added': 384,\n",
       " 'fat': 385,\n",
       " 'liked': 386,\n",
       " 'blend': 387,\n",
       " 'arrived': 388,\n",
       " 'went': 389,\n",
       " 'ok': 390,\n",
       " 'pieces': 391,\n",
       " 'side': 392,\n",
       " \"wouldn't\": 393,\n",
       " 'cost': 394,\n",
       " 'variety': 395,\n",
       " 'syrup': 396,\n",
       " 'us': 397,\n",
       " 'perfect': 398,\n",
       " 'brands': 399,\n",
       " 'where': 400,\n",
       " 'fact': 401,\n",
       " 'ounce': 402,\n",
       " 'opened': 403,\n",
       " 'prefer': 404,\n",
       " \"they're\": 405,\n",
       " 'couple': 406,\n",
       " 'either': 407,\n",
       " 'grocery': 408,\n",
       " 'ones': 409,\n",
       " 'dark': 410,\n",
       " 'husband': 411,\n",
       " 'others': 412,\n",
       " 'else': 413,\n",
       " 'wheat': 414,\n",
       " 'peanut': 415,\n",
       " 'guess': 416,\n",
       " 'beans': 417,\n",
       " 'overall': 418,\n",
       " 'roast': 419,\n",
       " 'serving': 420,\n",
       " 'care': 421,\n",
       " 'feed': 422,\n",
       " 'four': 423,\n",
       " 'decided': 424,\n",
       " 'especially': 425,\n",
       " 'whatever': 426,\n",
       " 'boxes': 427,\n",
       " 'cream': 428,\n",
       " 'hours': 429,\n",
       " 'keurig': 430,\n",
       " 'nuts': 431,\n",
       " 'feeding': 432,\n",
       " 'second': 433,\n",
       " 'cherry': 434,\n",
       " 'started': 435,\n",
       " 'family': 436,\n",
       " 'bottom': 437,\n",
       " 'mixed': 438,\n",
       " 'health': 439,\n",
       " 'unfortunately': 440,\n",
       " 'end': 441,\n",
       " 'wonderful': 442,\n",
       " 'fan': 443,\n",
       " 'granola': 444,\n",
       " 'inside': 445,\n",
       " 'recommended': 446,\n",
       " \"couldn't\": 447,\n",
       " 'light': 448,\n",
       " 'let': 449,\n",
       " 'http': 450,\n",
       " 'www': 451,\n",
       " 'weeks': 452,\n",
       " 'ago': 453,\n",
       " 'next': 454,\n",
       " 'says': 455,\n",
       " 'idea': 456,\n",
       " 'href': 457,\n",
       " 'gp': 458,\n",
       " 'must': 459,\n",
       " 'etc': 460,\n",
       " 'yet': 461,\n",
       " 'cheap': 462,\n",
       " 'making': 463,\n",
       " 'home': 464,\n",
       " 'drinking': 465,\n",
       " 'although': 466,\n",
       " 'total': 467,\n",
       " 'please': 468,\n",
       " 'read': 469,\n",
       " 'part': 470,\n",
       " 'deal': 471,\n",
       " 'bold': 472,\n",
       " 'color': 473,\n",
       " 'spicy': 474,\n",
       " '50': 475,\n",
       " 'longer': 476,\n",
       " 'tell': 477,\n",
       " 'minutes': 478,\n",
       " 'popcorn': 479,\n",
       " 'type': 480,\n",
       " 'likes': 481,\n",
       " 'stick': 482,\n",
       " 'excellent': 483,\n",
       " 'stores': 484,\n",
       " 'wish': 485,\n",
       " 'white': 486,\n",
       " 'experience': 487,\n",
       " 'soft': 488,\n",
       " 'waste': 489,\n",
       " 'cans': 490,\n",
       " 'shampoo': 491,\n",
       " 'month': 492,\n",
       " 'weak': 493,\n",
       " 'left': 494,\n",
       " 'believe': 495,\n",
       " 'star': 496,\n",
       " 'artificial': 497,\n",
       " 'extra': 498,\n",
       " 'clean': 499,\n",
       " 'slightly': 500,\n",
       " 'stale': 501,\n",
       " 'list': 502,\n",
       " 'wrong': 503,\n",
       " 'brew': 504,\n",
       " 'anyone': 505,\n",
       " 'absolutely': 506,\n",
       " 'chews': 507,\n",
       " 'week': 508,\n",
       " 'brown': 509,\n",
       " 'reason': 510,\n",
       " 'looks': 511,\n",
       " 'date': 512,\n",
       " 'ground': 513,\n",
       " 'difference': 514,\n",
       " 'version': 515,\n",
       " 'red': 516,\n",
       " 'works': 517,\n",
       " 'cheaper': 518,\n",
       " 'twice': 519,\n",
       " 'cookie': 520,\n",
       " 'help': 521,\n",
       " 'quickly': 522,\n",
       " 'contains': 523,\n",
       " 'pods': 524,\n",
       " 'meat': 525,\n",
       " 'container': 526,\n",
       " 'kids': 527,\n",
       " 'acid': 528,\n",
       " 'aftertaste': 529,\n",
       " 'apple': 530,\n",
       " 'itself': 531,\n",
       " 'told': 532,\n",
       " 'cannot': 533,\n",
       " 'jar': 534,\n",
       " 'honey': 535,\n",
       " 'soy': 536,\n",
       " 'ingredient': 537,\n",
       " 'ginger': 538,\n",
       " '100': 539,\n",
       " 'bean': 540,\n",
       " 'myself': 541,\n",
       " 'cut': 542,\n",
       " 'highly': 543,\n",
       " 'choice': 544,\n",
       " 'ordering': 545,\n",
       " 'salty': 546,\n",
       " 'cheese': 547,\n",
       " 'hungry': 548,\n",
       " 'single': 549,\n",
       " 'chew': 550,\n",
       " 'non': 551,\n",
       " 'plus': 552,\n",
       " 'available': 553,\n",
       " 'sodium': 554,\n",
       " 'ate': 555,\n",
       " 'original': 556,\n",
       " '7': 557,\n",
       " 'gift': 558,\n",
       " 'expect': 559,\n",
       " 'bite': 560,\n",
       " 'bowl': 561,\n",
       " 'rest': 562,\n",
       " 'sour': 563,\n",
       " 'machine': 564,\n",
       " 'rope': 565,\n",
       " 'wellness': 566,\n",
       " 'seemed': 567,\n",
       " \"aren't\": 568,\n",
       " 'smooth': 569,\n",
       " 'soda': 570,\n",
       " 'easily': 571,\n",
       " 'baby': 572,\n",
       " 'huge': 573,\n",
       " 'someone': 574,\n",
       " 'amazing': 575,\n",
       " 'ball': 576,\n",
       " 'items': 577,\n",
       " 'lower': 578,\n",
       " 'okay': 579,\n",
       " 'expected': 580,\n",
       " 'normal': 581,\n",
       " 'results': 582,\n",
       " 'close': 583,\n",
       " 'done': 584,\n",
       " 'vet': 585,\n",
       " 'awful': 586,\n",
       " 'description': 587,\n",
       " 'dried': 588,\n",
       " 'crunchy': 589,\n",
       " 'online': 590,\n",
       " 'market': 591,\n",
       " 'completely': 592,\n",
       " 'life': 593,\n",
       " 'needs': 594,\n",
       " 'place': 595,\n",
       " 'smells': 596,\n",
       " 'start': 597,\n",
       " 'aroma': 598,\n",
       " 'based': 599,\n",
       " 'formula': 600,\n",
       " \"haven't\": 601,\n",
       " 'finally': 602,\n",
       " 'similar': 603,\n",
       " 'body': 604,\n",
       " 'snacks': 605,\n",
       " 'label': 606,\n",
       " 'flour': 607,\n",
       " 'value': 608,\n",
       " 'name': 609,\n",
       " 'five': 610,\n",
       " 'mind': 611,\n",
       " 'felt': 612,\n",
       " 'fiber': 613,\n",
       " 'starbucks': 614,\n",
       " 'certainly': 615,\n",
       " 'teas': 616,\n",
       " 'problems': 617,\n",
       " 'smaller': 618,\n",
       " 'often': 619,\n",
       " 'enjoyed': 620,\n",
       " 'giving': 621,\n",
       " 'grain': 622,\n",
       " 'saw': 623,\n",
       " 'pay': 624,\n",
       " 'given': 625,\n",
       " 'french': 626,\n",
       " 'excited': 627,\n",
       " 'course': 628,\n",
       " 'blue': 629,\n",
       " 'figured': 630,\n",
       " 'hazelnut': 631,\n",
       " 'expecting': 632,\n",
       " 'surprised': 633,\n",
       " 'due': 634,\n",
       " 'change': 635,\n",
       " 'along': 636,\n",
       " 'hoping': 637,\n",
       " 'lemon': 638,\n",
       " 'son': 639,\n",
       " 'cinnamon': 640,\n",
       " 'perhaps': 641,\n",
       " 'everything': 642,\n",
       " 'pepper': 643,\n",
       " \"there's\": 644,\n",
       " 'pod': 645,\n",
       " 'breakfast': 646,\n",
       " 'tiny': 647,\n",
       " 'benecol': 648,\n",
       " 'teeth': 649,\n",
       " 'between': 650,\n",
       " 'meal': 651,\n",
       " 'yourself': 652,\n",
       " \"you'll\": 653,\n",
       " 'contain': 654,\n",
       " 'later': 655,\n",
       " 'doing': 656,\n",
       " 'leave': 657,\n",
       " 'heat': 658,\n",
       " 'plain': 659,\n",
       " 'special': 660,\n",
       " 'fast': 661,\n",
       " 'decaf': 662,\n",
       " 'alternative': 663,\n",
       " 'anyway': 664,\n",
       " 'past': 665,\n",
       " 'canned': 666,\n",
       " '20': 667,\n",
       " 'taking': 668,\n",
       " 'caffeine': 669,\n",
       " 'leaves': 670,\n",
       " 'clear': 671,\n",
       " 'unless': 672,\n",
       " 'line': 673,\n",
       " 'during': 674,\n",
       " 'called': 675,\n",
       " 'peppermint': 676,\n",
       " 'terrible': 677,\n",
       " 'brewing': 678,\n",
       " 'drinks': 679,\n",
       " 'figure': 680,\n",
       " 'wet': 681,\n",
       " 'almonds': 682,\n",
       " 'already': 683,\n",
       " 'looked': 684,\n",
       " 'bottles': 685,\n",
       " 'eaten': 686,\n",
       " 'subscribe': 687,\n",
       " 'broken': 688,\n",
       " 'everyone': 689,\n",
       " 'decent': 690,\n",
       " '30': 691,\n",
       " 'hand': 692,\n",
       " 'stomach': 693,\n",
       " 'quick': 694,\n",
       " 'exactly': 695,\n",
       " 'picture': 696,\n",
       " 'friends': 697,\n",
       " 'gets': 698,\n",
       " 'extremely': 699,\n",
       " 'sorry': 700,\n",
       " 'weight': 701,\n",
       " 'cocoa': 702,\n",
       " 'compared': 703,\n",
       " 'daughter': 704,\n",
       " 'seller': 705,\n",
       " 'eats': 706,\n",
       " 'sent': 707,\n",
       " 'opinion': 708,\n",
       " 'live': 709,\n",
       " 'orange': 710,\n",
       " 'except': 711,\n",
       " 'difficult': 712,\n",
       " 'return': 713,\n",
       " 'coffees': 714,\n",
       " 'hope': 715,\n",
       " '16': 716,\n",
       " 'hint': 717,\n",
       " 'packages': 718,\n",
       " 'goes': 719,\n",
       " 'avoid': 720,\n",
       " 'ended': 721,\n",
       " 'super': 722,\n",
       " 'noticed': 723,\n",
       " 'mine': 724,\n",
       " 'medium': 725,\n",
       " 'cause': 726,\n",
       " 'point': 727,\n",
       " 'gum': 728,\n",
       " 'supposed': 729,\n",
       " 'needed': 730,\n",
       " 'liquid': 731,\n",
       " 'under': 732,\n",
       " 'crazy': 733,\n",
       " 'stop': 734,\n",
       " 'wife': 735,\n",
       " 'stuck': 736,\n",
       " 'paid': 737,\n",
       " 'kibble': 738,\n",
       " 'horrible': 739,\n",
       " 'purchasing': 740,\n",
       " 'sometimes': 741,\n",
       " 'soon': 742,\n",
       " 'anymore': 743,\n",
       " 'rich': 744,\n",
       " 'finish': 745,\n",
       " 'glad': 746,\n",
       " 'content': 747,\n",
       " 'cake': 748,\n",
       " 'chili': 749,\n",
       " 'customer': 750,\n",
       " 'entire': 751,\n",
       " 'friend': 752,\n",
       " 'servings': 753,\n",
       " 'loose': 754,\n",
       " 'floor': 755,\n",
       " 'larger': 756,\n",
       " 'beef': 757,\n",
       " 'pound': 758,\n",
       " 'mild': 759,\n",
       " 'gone': 760,\n",
       " 'grams': 761,\n",
       " 'vitamin': 762,\n",
       " 'yes': 763,\n",
       " 'skin': 764,\n",
       " 'otherwise': 765,\n",
       " 'actual': 766,\n",
       " 'spice': 767,\n",
       " 'service': 768,\n",
       " 'batch': 769,\n",
       " 'lots': 770,\n",
       " 'glass': 771,\n",
       " 'short': 772,\n",
       " 'pouches': 773,\n",
       " '9': 774,\n",
       " 'wash': 775,\n",
       " '24': 776,\n",
       " 'worst': 777,\n",
       " 'filling': 778,\n",
       " 'somewhat': 779,\n",
       " 'paper': 780,\n",
       " 'pill': 781,\n",
       " 'gf': 782,\n",
       " 'brewed': 783,\n",
       " 'mess': 784,\n",
       " 'adding': 785,\n",
       " 'yummy': 786,\n",
       " 'baking': 787,\n",
       " 'percent': 788,\n",
       " '00': 789,\n",
       " 'fit': 790,\n",
       " 'sold': 791,\n",
       " 'vinegar': 792,\n",
       " 'thick': 793,\n",
       " 'call': 794,\n",
       " 'worse': 795,\n",
       " 'scent': 796,\n",
       " 'normally': 797,\n",
       " 'run': 798,\n",
       " 'flavorful': 799,\n",
       " 'basically': 800,\n",
       " 'nutrition': 801,\n",
       " 'sealed': 802,\n",
       " '0': 803,\n",
       " 'delivery': 804,\n",
       " 'fish': 805,\n",
       " 'issue': 806,\n",
       " 'packaged': 807,\n",
       " 'means': 808,\n",
       " '99': 809,\n",
       " 'recently': 810,\n",
       " 'changed': 811,\n",
       " 'picky': 812,\n",
       " 'pleased': 813,\n",
       " 'packs': 814,\n",
       " 'piece': 815,\n",
       " 'continue': 816,\n",
       " 'reading': 817,\n",
       " 'shipped': 818,\n",
       " 'grains': 819,\n",
       " 'thinking': 820,\n",
       " 'smart': 821,\n",
       " 'throw': 822,\n",
       " 'instant': 823,\n",
       " 'simply': 824,\n",
       " 'mango': 825,\n",
       " 'none': 826,\n",
       " 'chewing': 827,\n",
       " 'salad': 828,\n",
       " 'maker': 829,\n",
       " 'daily': 830,\n",
       " 'c': 831,\n",
       " 'hour': 832,\n",
       " 'calorie': 833,\n",
       " 'seen': 834,\n",
       " 'potato': 835,\n",
       " 'higher': 836,\n",
       " 'became': 837,\n",
       " 'oatmeal': 838,\n",
       " 'issues': 839,\n",
       " 'particular': 840,\n",
       " 'seeds': 841,\n",
       " 'thin': 842,\n",
       " '15': 843,\n",
       " 'garbage': 844,\n",
       " 'within': 845,\n",
       " 'caramel': 846,\n",
       " 'mint': 847,\n",
       " 'pop': 848,\n",
       " 'varieties': 849,\n",
       " 'listed': 850,\n",
       " 'mostly': 851,\n",
       " 'note': 852,\n",
       " 'form': 853,\n",
       " 'touch': 854,\n",
       " 'blueberry': 855,\n",
       " 'hold': 856,\n",
       " \"what's\": 857,\n",
       " 'disappointment': 858,\n",
       " 'reviewers': 859,\n",
       " 'usual': 860,\n",
       " 'sweetness': 861,\n",
       " 'bland': 862,\n",
       " 'true': 863,\n",
       " 'together': 864,\n",
       " 'above': 865,\n",
       " 'stay': 866,\n",
       " 'website': 867,\n",
       " 'heavy': 868,\n",
       " 'level': 869,\n",
       " 'packet': 870,\n",
       " 'mountain': 871,\n",
       " 'matter': 872,\n",
       " 'crunch': 873,\n",
       " 'types': 874,\n",
       " 'almond': 875,\n",
       " 'pouch': 876,\n",
       " 'creamy': 877,\n",
       " 'filter': 878,\n",
       " 'feeling': 879,\n",
       " 'grape': 880,\n",
       " 'notice': 881,\n",
       " 'nasty': 882,\n",
       " 'dinner': 883,\n",
       " 'stronger': 884,\n",
       " 'totally': 885,\n",
       " 'pleasant': 886,\n",
       " 'putting': 887,\n",
       " 'saying': 888,\n",
       " '11': 889,\n",
       " 'pasta': 890,\n",
       " 'oh': 891,\n",
       " 'serve': 892,\n",
       " 'chemical': 893,\n",
       " 'ice': 894,\n",
       " 'plenty': 895,\n",
       " 'count': 896,\n",
       " 'important': 897,\n",
       " 'chai': 898,\n",
       " 'consistency': 899,\n",
       " 'pet': 900,\n",
       " 'strawberry': 901,\n",
       " 'china': 902,\n",
       " 'sale': 903,\n",
       " 'expiration': 904,\n",
       " 'hate': 905,\n",
       " 'thank': 906,\n",
       " 'clusters': 907,\n",
       " 'sitting': 908,\n",
       " 'sweetener': 909,\n",
       " 'immediately': 910,\n",
       " 'recipe': 911,\n",
       " 'bulk': 912,\n",
       " 'chewy': 913,\n",
       " 'wait': 914,\n",
       " 'mean': 915,\n",
       " 'door': 916,\n",
       " 'adult': 917,\n",
       " 'shipment': 918,\n",
       " 'substitute': 919,\n",
       " 'sticks': 920,\n",
       " 'gives': 921,\n",
       " 'turn': 922,\n",
       " 'opening': 923,\n",
       " 'sort': 924,\n",
       " 'crackers': 925,\n",
       " 'nearly': 926,\n",
       " 'forward': 927,\n",
       " 'six': 928,\n",
       " 'packets': 929,\n",
       " 'remember': 930,\n",
       " 'toys': 931,\n",
       " 'candies': 932,\n",
       " 'scalp': 933,\n",
       " 'hit': 934,\n",
       " 'ounces': 935,\n",
       " 'awesome': 936,\n",
       " 'including': 937,\n",
       " 'christmas': 938,\n",
       " 'kit': 939,\n",
       " 'peanuts': 940,\n",
       " 'generally': 941,\n",
       " 'filled': 942,\n",
       " 'vitamins': 943,\n",
       " 'e': 944,\n",
       " 'rating': 945,\n",
       " 'check': 946,\n",
       " 'understand': 947,\n",
       " \"we've\": 948,\n",
       " 'consider': 949,\n",
       " 'mustard': 950,\n",
       " 'nor': 951,\n",
       " 'front': 952,\n",
       " 'lick': 953,\n",
       " 'lid': 954,\n",
       " 'general': 955,\n",
       " 'reviewer': 956,\n",
       " 'overly': 957,\n",
       " 'yogurt': 958,\n",
       " 'bits': 959,\n",
       " 'litter': 960,\n",
       " 'lost': 961,\n",
       " 'pure': 962,\n",
       " 'working': 963,\n",
       " 'obviously': 964,\n",
       " 'nutritional': 965,\n",
       " '25': 966,\n",
       " 'convenient': 967,\n",
       " 'process': 968,\n",
       " 'flavoring': 969,\n",
       " 'iced': 970,\n",
       " 'worked': 971,\n",
       " 'egg': 972,\n",
       " 'sample': 973,\n",
       " 'person': 974,\n",
       " 'began': 975,\n",
       " 'neither': 976,\n",
       " 'happened': 977,\n",
       " 'salmon': 978,\n",
       " 'possible': 979,\n",
       " 'middle': 980,\n",
       " 'indoor': 981,\n",
       " 'followed': 982,\n",
       " 'mocha': 983,\n",
       " 'mistake': 984,\n",
       " 'job': 985,\n",
       " 'jars': 986,\n",
       " 'jerky': 987,\n",
       " 'chunks': 988,\n",
       " 'switch': 989,\n",
       " 'style': 990,\n",
       " 'play': 991,\n",
       " 'dressing': 992,\n",
       " 'asked': 993,\n",
       " 'takes': 994,\n",
       " 'tongue': 995,\n",
       " 'easier': 996,\n",
       " \"he's\": 997,\n",
       " 'thanks': 998,\n",
       " 'ship': 999,\n",
       " 'delivered': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer converts all the words to lower case, removes most of the separators by default (unless specific ones are chosen) and assigns a value to each word, most likely using the tf-idf method to order the words in the returned dictionary. Therefore, we can see that the word_index method returns the words and they are order from most common to less common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 34, 268, 40, 40, 29]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['I just feel very very good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make have simply little think rabbits beer eggwhite']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[109, 19, 824, 76, 114, 6315, 1137, 8070]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "The texts_to_sequences method simply matches each word to its token. Returning the integer value paired to each word if it exists in the tokenizer. Conversely, the sequences_to_texts takes list of integers and returns the words matched to each of those integers, if they exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3\n",
    "\n",
    "Use the tokenizer to transform the texts in our test and train data to sequences. Then, use the `pad_sequences` function to pad/truncate these sequences to length 116 (the 80th percentile of text lengths). Save the resulting arrays as `train_sequences` and `test_sequences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(rev_train.Text)\n",
    "test_seq = tokenizer.texts_to_sequences(rev_test.Text)\n",
    "train_sequences = pad_sequences(\n",
    "    train_seq, maxlen=116, dtype='int32', padding='pre', truncating='pre',\n",
    "    value=0.0)\n",
    "test_sequences = pad_sequences(\n",
    "    test_seq, maxlen=116, dtype='int32', padding='pre', truncating='pre',\n",
    "    value=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = rev_train.Score\n",
    "test_labels = rev_test.Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic neural network model \n",
    "\n",
    "Now that we have preprocessed the text, let's create a basic neural network to train on our data. We'll use an embedding layer which performs [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) on our word sequences, two fully connected (\"dense\") layers, and an output layer with 5 neurons to represent the 5 possible star ratings.\n",
    "\n",
    "Before we train a `keras` model, there is an additional `compile` step where we define what loss function and optimizer to use, and what metrics to output. Then we can train the model using the `fit` function. All of this is shown below.\n",
    "\n",
    "Note the `validation_split=0.2` argument which tells Keras to train on only 80% of the training data and tune the model on the remaining 20%, which we call the validation set. You can see the accuracy and loss for both the training and validation set in the output for each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 116, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,593,669\n",
      "Trainable params: 2,593,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.5884 - accuracy: 0.3050 - val_loss: 1.5270 - val_accuracy: 0.4025\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.3286 - accuracy: 0.4875 - val_loss: 1.2604 - val_accuracy: 0.4663\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.9667 - accuracy: 0.6456 - val_loss: 1.2167 - val_accuracy: 0.4863\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.6143 - accuracy: 0.8044 - val_loss: 1.3429 - val_accuracy: 0.4863\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3197 - accuracy: 0.9150 - val_loss: 1.5248 - val_accuracy: 0.4762\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1440 - accuracy: 0.9734 - val_loss: 1.7319 - val_accuracy: 0.4725\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0628 - accuracy: 0.9937 - val_loss: 1.9457 - val_accuracy: 0.4737\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0288 - accuracy: 0.9987 - val_loss: 2.1280 - val_accuracy: 0.4737\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 2.2195 - val_accuracy: 0.4775\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 2.2785 - val_accuracy: 0.4638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "How well does this model perform? How does this compare to a baseline expectation? What do you notice about the accuracy and loss values for both the validation and training sets over time and what does this mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Model performance')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = history.history\n",
    "# print(data)\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "ax = sns.lineplot(x=range(1,11), y=data[\"loss\"])\n",
    "ax = sns.lineplot(x=range(1,11), y=data[\"accuracy\"])\n",
    "ax = sns.lineplot(x=range(1,11), y=data[\"val_loss\"])\n",
    "ax = sns.lineplot(x=range(1,11), y=data[\"val_accuracy\"])\n",
    "ax.legend(['loss',\"accuracy\",\"val_loss\",\"val_accuracy\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "We can see that this model performs extremely well on the train set but not so much on the validation set. In fact, the accuracy even after several runs does not change much. Also, the loss of the train set decreases continuously down to zero while the loss of the validation quickly reaches a minimum at the 3rd epoch and then starts increasing again. This is a signal that the model is not learning from that epoch forward. We could say that the model does not perform very well. It could also be a signal of overfitting of the model on the train dataset and that is why it underperforms after the 3rd epoch.\n",
    "\n",
    "Nevertheless, if we think of a baseline model that predicts all reviews with the same Score, we would have a baseline accuracy of around 20% because we defined 5000 as the whole dataset, 4000 for training the model and 3200/800 for train/val. A baseline model would correctly predict 1000 scores correctly. Currently, the model would be predicting roughly 50% at the validation level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with different regularization strategies\n",
    "\n",
    "There are many different ways to mitigate overfitting in a neural network, collectively known as *regularization* techniques. One common regularization technique is called [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout). In this regularization method, a set of neurons is randomly selected at each training step to be completely ignored. This is done so that the neurons in our network do not rely strongly on their neighboring neurons and we avoid the creation of [\"co-adaptations\"](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) that do not generalize well to unseen data. This making the model more robust and less prone to overffiting.\n",
    "\n",
    "You can create dropouts in `keras` by adding a layer named `Dropout(p)`, where `p` is the probability of dropping neurons in the previous layer. For example, the following model would implement dropout by removing roughly 20% percent of the outputs of the embedding layer at each training step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5939 - accuracy: 0.2666 - val_loss: 1.5480 - val_accuracy: 0.3150\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.3989 - accuracy: 0.4075 - val_loss: 1.3101 - val_accuracy: 0.4375\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.1285 - accuracy: 0.5581 - val_loss: 1.2332 - val_accuracy: 0.4775\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.8419 - accuracy: 0.6853 - val_loss: 1.2571 - val_accuracy: 0.4850\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5619 - accuracy: 0.8172 - val_loss: 1.3132 - val_accuracy: 0.5025\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3340 - accuracy: 0.9056 - val_loss: 1.4707 - val_accuracy: 0.4913\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1799 - accuracy: 0.9647 - val_loss: 1.6781 - val_accuracy: 0.5025\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0912 - accuracy: 0.9853 - val_loss: 1.8707 - val_accuracy: 0.4988\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0483 - accuracy: 0.9956 - val_loss: 2.0354 - val_accuracy: 0.4913\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0272 - accuracy: 0.9981 - val_loss: 2.1127 - val_accuracy: 0.4963\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_0 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Modify the neural network definition above to try and fix the overfitting problem using Dropout. Explain the configuration that you tried and your results. Why do you think your modifications were or were not able to mitigate the overfitting problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 1.5983 - accuracy: 0.2444 - val_loss: 1.5727 - val_accuracy: 0.3075\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.4749 - accuracy: 0.3644 - val_loss: 1.3903 - val_accuracy: 0.4025\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.2442 - accuracy: 0.4834 - val_loss: 1.2606 - val_accuracy: 0.4675\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.0204 - accuracy: 0.5931 - val_loss: 1.2282 - val_accuracy: 0.4675\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.8115 - accuracy: 0.6966 - val_loss: 1.2667 - val_accuracy: 0.4825\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.5961 - accuracy: 0.7953 - val_loss: 1.3502 - val_accuracy: 0.4762\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.4162 - accuracy: 0.8600 - val_loss: 1.4574 - val_accuracy: 0.4837\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.2773 - accuracy: 0.9156 - val_loss: 1.6314 - val_accuracy: 0.4700\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1877 - accuracy: 0.9425 - val_loss: 1.8330 - val_accuracy: 0.4525\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1160 - accuracy: 0.9722 - val_loss: 2.0118 - val_accuracy: 0.4812\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_1 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5940 - accuracy: 0.2594 - val_loss: 1.5612 - val_accuracy: 0.3550\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.3959 - accuracy: 0.4384 - val_loss: 1.2918 - val_accuracy: 0.4613\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.0775 - accuracy: 0.5794 - val_loss: 1.2184 - val_accuracy: 0.4888\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.8068 - accuracy: 0.6994 - val_loss: 1.2161 - val_accuracy: 0.4863\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.5462 - accuracy: 0.8181 - val_loss: 1.3182 - val_accuracy: 0.5025\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.3153 - accuracy: 0.9159 - val_loss: 1.4761 - val_accuracy: 0.5138\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1725 - accuracy: 0.9581 - val_loss: 1.6685 - val_accuracy: 0.4913\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.0954 - accuracy: 0.9803 - val_loss: 1.9004 - val_accuracy: 0.4888\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.0486 - accuracy: 0.9956 - val_loss: 2.0293 - val_accuracy: 0.4988\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 0.0295 - accuracy: 0.9962 - val_loss: 2.1606 - val_accuracy: 0.4913\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_2 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 1.6005 - accuracy: 0.2500 - val_loss: 1.5768 - val_accuracy: 0.3113\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 1.4725 - accuracy: 0.3684 - val_loss: 1.3657 - val_accuracy: 0.4175\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.2347 - accuracy: 0.4875 - val_loss: 1.2437 - val_accuracy: 0.4563\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.0122 - accuracy: 0.6112 - val_loss: 1.2225 - val_accuracy: 0.4775\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.7841 - accuracy: 0.7103 - val_loss: 1.2767 - val_accuracy: 0.4850\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.5733 - accuracy: 0.8075 - val_loss: 1.3779 - val_accuracy: 0.4762\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.4062 - accuracy: 0.8659 - val_loss: 1.5384 - val_accuracy: 0.4762\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 0.2862 - accuracy: 0.9116 - val_loss: 1.6593 - val_accuracy: 0.4812\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1893 - accuracy: 0.9444 - val_loss: 1.8355 - val_accuracy: 0.4900\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1267 - accuracy: 0.9656 - val_loss: 1.9880 - val_accuracy: 0.4913\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_3 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5967 - accuracy: 0.2553 - val_loss: 1.5584 - val_accuracy: 0.3313\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.4298 - accuracy: 0.4122 - val_loss: 1.3261 - val_accuracy: 0.4387\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.1858 - accuracy: 0.5206 - val_loss: 1.2328 - val_accuracy: 0.4625\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.9509 - accuracy: 0.6297 - val_loss: 1.2506 - val_accuracy: 0.4812\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.7137 - accuracy: 0.7391 - val_loss: 1.3276 - val_accuracy: 0.4837\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.4930 - accuracy: 0.8359 - val_loss: 1.4589 - val_accuracy: 0.4575\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.3279 - accuracy: 0.8966 - val_loss: 1.6328 - val_accuracy: 0.4575\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.2199 - accuracy: 0.9356 - val_loss: 1.8240 - val_accuracy: 0.4475\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1431 - accuracy: 0.9597 - val_loss: 2.0124 - val_accuracy: 0.4550\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.0891 - accuracy: 0.9784 - val_loss: 2.2010 - val_accuracy: 0.4425\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_4 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5968 - accuracy: 0.2481 - val_loss: 1.5685 - val_accuracy: 0.3050\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.4526 - accuracy: 0.3700 - val_loss: 1.3839 - val_accuracy: 0.3988\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.2098 - accuracy: 0.5116 - val_loss: 1.2720 - val_accuracy: 0.4500\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.9823 - accuracy: 0.6100 - val_loss: 1.2496 - val_accuracy: 0.4700\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.7515 - accuracy: 0.7294 - val_loss: 1.2791 - val_accuracy: 0.4775\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.5318 - accuracy: 0.8300 - val_loss: 1.3728 - val_accuracy: 0.4787\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.3580 - accuracy: 0.8856 - val_loss: 1.4944 - val_accuracy: 0.4787\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.2279 - accuracy: 0.9331 - val_loss: 1.6608 - val_accuracy: 0.4400\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1419 - accuracy: 0.9603 - val_loss: 1.8140 - val_accuracy: 0.4600\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.0854 - accuracy: 0.9778 - val_loss: 1.9576 - val_accuracy: 0.4588\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.3)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.3))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_4_5 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "From the 5 models tried, we can see that history_4_2 has the highest validation accuracy during validation: 51.38%. Slightly higher than the best validation accuracy (50.50%) for the first model with dropout (history_4_0). This results might not be significantly different, as the dropout turns off randomly a defined percentage of the neurons and not always the same. \n",
    "\n",
    "It is noticeable how using a dropout over 0.2 tends to decrease the performance of the model overall. This is likely to result in giving too much load to few neurons that cannot generalize correctly for the validation dataset.\n",
    "\n",
    "Also, the idea of the best performing model comes from the fact that setting a lower (0.1) dropout in two consecutive layers means that every epoch, these two layers will have different neurons to fit the data and will force the network to better generalize for the fewer neurons and connections. This is also seen in the paper of Srivastava, where the dropout networks that outperform tend to have dropouts in several layers. The accuracy, nevertheless, does not improve greatly with this technique.\n",
    "\n",
    "In my opinion, the model is not improving greatly either because there is simply not enough data or the data is very similar and the model cannot avoid overfitting with the parameters as have been set up so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Keras allows you to add [L1](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1), [L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2), or [L1 and L2](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1_l2) combined regularizers on individual layers by passing in the `kernel_regularizer`, `bias_regularizer` or `activity_regularizer` arguments. In neural networks, these regularizers work by penalizing the loss function in different ways, based on the number of weights or the size of the weights.\n",
    "\n",
    "Try 4-5 different combinations of L1, L2, L1 and L2 regularization in different combinations on different layers. In each example, explain why you tried that configuration and the results. Why do you think your modifications were or were not able to mitigate the overfitting problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.6212 - accuracy: 0.2731 - val_loss: 1.5921 - val_accuracy: 0.3837\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.4272 - accuracy: 0.4422 - val_loss: 1.3120 - val_accuracy: 0.4762\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.1058 - accuracy: 0.5972 - val_loss: 1.2652 - val_accuracy: 0.4800\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8236 - accuracy: 0.7153 - val_loss: 1.3222 - val_accuracy: 0.4712\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5619 - accuracy: 0.8247 - val_loss: 1.4474 - val_accuracy: 0.4737\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3503 - accuracy: 0.9078 - val_loss: 1.6396 - val_accuracy: 0.4638\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2179 - accuracy: 0.9550 - val_loss: 1.8711 - val_accuracy: 0.4613\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1436 - accuracy: 0.9819 - val_loss: 2.0276 - val_accuracy: 0.4625\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1052 - accuracy: 0.9909 - val_loss: 2.0970 - val_accuracy: 0.4762\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0824 - accuracy: 0.9969 - val_loss: 2.2084 - val_accuracy: 0.4762\n"
     ]
    }
   ],
   "source": [
    "# regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "# regularizers.l1(l=0.01)\n",
    "# regularizers.l2(l=0.01)\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l2(l=0.0001))) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l2(l=0.0001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_0 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.8016 - accuracy: 0.2416 - val_loss: 1.7234 - val_accuracy: 0.3200\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.6221 - accuracy: 0.3613 - val_loss: 1.5064 - val_accuracy: 0.4225\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.3710 - accuracy: 0.4684 - val_loss: 1.4223 - val_accuracy: 0.4288\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.1653 - accuracy: 0.5809 - val_loss: 1.4407 - val_accuracy: 0.4350\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.9565 - accuracy: 0.6769 - val_loss: 1.5584 - val_accuracy: 0.4450\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.7711 - accuracy: 0.7672 - val_loss: 1.6583 - val_accuracy: 0.4538\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.6089 - accuracy: 0.8356 - val_loss: 1.8562 - val_accuracy: 0.4700\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.4739 - accuracy: 0.8891 - val_loss: 2.0636 - val_accuracy: 0.4512\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3747 - accuracy: 0.9250 - val_loss: 2.2081 - val_accuracy: 0.4487\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3192 - accuracy: 0.9463 - val_loss: 2.3661 - val_accuracy: 0.4575\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l1_l2(l1=0.0001, l2=0.0001))) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu', kernel_regularizer = regularizers.l1_l2(l1=0.0001, l2=0.0001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_1 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.5902 - accuracy: 0.2678 - val_loss: 1.5411 - val_accuracy: 0.3512\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.3744 - accuracy: 0.4338 - val_loss: 1.2920 - val_accuracy: 0.4675\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.0640 - accuracy: 0.5925 - val_loss: 1.2247 - val_accuracy: 0.4650\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.7599 - accuracy: 0.7247 - val_loss: 1.2743 - val_accuracy: 0.4837\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.4806 - accuracy: 0.8462 - val_loss: 1.3971 - val_accuracy: 0.4762\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2790 - accuracy: 0.9212 - val_loss: 1.6037 - val_accuracy: 0.4613\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1408 - accuracy: 0.9741 - val_loss: 1.8076 - val_accuracy: 0.4650\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0751 - accuracy: 0.9891 - val_loss: 1.9765 - val_accuracy: 0.4600\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0418 - accuracy: 0.9953 - val_loss: 2.0972 - val_accuracy: 0.4663\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0229 - accuracy: 0.9994 - val_loss: 2.3264 - val_accuracy: 0.4638\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', bias_regularizer = regularizers.l2(l=0.0001))) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu', bias_regularizer = regularizers.l2(l=0.0001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_2 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5845 - accuracy: 0.2763 - val_loss: 1.5095 - val_accuracy: 0.3700\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.3783 - accuracy: 0.4428 - val_loss: 1.3293 - val_accuracy: 0.4062\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.0925 - accuracy: 0.5756 - val_loss: 1.2504 - val_accuracy: 0.4650\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8112 - accuracy: 0.6966 - val_loss: 1.3179 - val_accuracy: 0.4650\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5327 - accuracy: 0.8228 - val_loss: 1.4176 - val_accuracy: 0.4737\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3139 - accuracy: 0.9106 - val_loss: 1.6133 - val_accuracy: 0.4663\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1714 - accuracy: 0.9559 - val_loss: 1.7914 - val_accuracy: 0.4675\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0896 - accuracy: 0.9825 - val_loss: 1.9848 - val_accuracy: 0.4837\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0439 - accuracy: 0.9959 - val_loss: 2.1137 - val_accuracy: 0.4787\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0250 - accuracy: 0.9987 - val_loss: 2.2421 - val_accuracy: 0.4712\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', bias_regularizer = regularizers.l1_l2(l1=0.00001, l2=0.0001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu', bias_regularizer = regularizers.l1_l2(l1=0.00001, l2=0.0001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_3 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5996 - accuracy: 0.2800 - val_loss: 1.5633 - val_accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.4266 - accuracy: 0.4419 - val_loss: 1.3537 - val_accuracy: 0.4450\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.1597 - accuracy: 0.5678 - val_loss: 1.2721 - val_accuracy: 0.4762\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8749 - accuracy: 0.7075 - val_loss: 1.3115 - val_accuracy: 0.4775\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.5804 - accuracy: 0.8537 - val_loss: 1.4201 - val_accuracy: 0.4800\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3910 - accuracy: 0.9291 - val_loss: 1.5267 - val_accuracy: 0.4725\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2702 - accuracy: 0.9737 - val_loss: 1.6304 - val_accuracy: 0.4688\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2091 - accuracy: 0.9878 - val_loss: 1.7074 - val_accuracy: 0.4712\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1709 - accuracy: 0.9934 - val_loss: 1.7790 - val_accuracy: 0.4625\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1497 - accuracy: 0.9959 - val_loss: 1.8198 - val_accuracy: 0.4600\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', activity_regularizer = regularizers.l2(l=0.001))) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu', activity_regularizer = regularizers.l2(l=0.001)))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_4 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.5815 - accuracy: 0.2894 - val_loss: 1.5133 - val_accuracy: 0.3625\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.3832 - accuracy: 0.4288 - val_loss: 1.3125 - val_accuracy: 0.4487\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.0974 - accuracy: 0.5628 - val_loss: 1.2335 - val_accuracy: 0.4900\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8055 - accuracy: 0.6997 - val_loss: 1.2793 - val_accuracy: 0.4888\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5270 - accuracy: 0.8291 - val_loss: 1.4288 - val_accuracy: 0.4712\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3204 - accuracy: 0.9100 - val_loss: 1.5851 - val_accuracy: 0.4600\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1788 - accuracy: 0.9616 - val_loss: 1.7904 - val_accuracy: 0.4538\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1054 - accuracy: 0.9822 - val_loss: 1.9681 - val_accuracy: 0.4500\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0589 - accuracy: 0.9953 - val_loss: 2.1026 - val_accuracy: 0.4550\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0401 - accuracy: 0.9981 - val_loss: 2.2331 - val_accuracy: 0.4575\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', activity_regularizer = regularizers.l1_l2(l1=0, l2=0.0001))) \n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_5 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.5939 - accuracy: 0.2756 - val_loss: 1.5525 - val_accuracy: 0.3963\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.3703 - accuracy: 0.4731 - val_loss: 1.2983 - val_accuracy: 0.4525\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.0140 - accuracy: 0.6306 - val_loss: 1.2705 - val_accuracy: 0.4775\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.6665 - accuracy: 0.7866 - val_loss: 1.3764 - val_accuracy: 0.4837\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.3743 - accuracy: 0.9091 - val_loss: 1.5576 - val_accuracy: 0.4775\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.1939 - accuracy: 0.9753 - val_loss: 1.7411 - val_accuracy: 0.4750\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1087 - accuracy: 0.9947 - val_loss: 1.8870 - val_accuracy: 0.4812\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.0741 - accuracy: 0.9991 - val_loss: 2.0142 - val_accuracy: 0.4737\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.0600 - accuracy: 0.9997 - val_loss: 2.0536 - val_accuracy: 0.4712\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.0526 - accuracy: 0.9997 - val_loss: 2.0989 - val_accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dense(128, activation='relu', activity_regularizer = regularizers.l1_l2(l1=0, l2=0.0001))) \n",
    "model2.add(Dense(128, activation='relu', activity_regularizer = regularizers.l1_l2(l1=0, l2=0.0001)))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_5_6 = model2.fit(train_sequences, train_labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "After adding different kinds of regularizations to the dense layers, in no case, there was an improvement over the model with dropout. It is important to note that i used the model with dropout in order to keep building upon the previous improvement.\n",
    "\n",
    "The regularizations L1 and L2 were chosen between 0 and 1e-5. This is given the recommendations in the literature as summarized in https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/. Also, applying L1 and L2 regularizations to the kernels, bias and activity did not show promising results, as the model kept obtaining similar results to the initial ones.\n",
    "\n",
    "More trials were done with values closer to 0 but for these values the regularization was so high that the network ended up not learning at all. Therefore, the impact was only seen with values between 0.001 and 0.0001.\n",
    "\n",
    "Also, regularizing only one layer instead of both showed the same results.\n",
    "\n",
    "When regularizing the model without the dropout, the results were very similar to adding the dropout only.\n",
    "\n",
    "Also, we could say that the regularization makes the nn more patience and avoid overfitting. Nevertheless, in this case, the nn seems to keep reaching a maximum of accuracy around 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization through adding more data\n",
    "\n",
    "Depending on the configurations you tried above, you probably saw that L1 and L2 regularization are pretty limited for this model and this amount of data. A more straightforward way to prevent overfitting is simply by adding more training data. If the network has more (and more varied) examples to learn from, perhaps it will learn more generalizable rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "How would you test the hypothesis that adding more data would result in a more generalizable model? Explain any change in results you see from further experimentation.\n",
    "\n",
    "**Hint:** Try adding 6000 reviews for each score instead. Compare with the original proposed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\envs\\extended_case_7\\lib\\site-packages\\pandas\\core\\generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "amazon_rev_reduced_6 = amazon_rev_sorted.groupby([\"Score\"]).head(6000)\n",
    "amazon_rev_reduced_6.Score = amazon_rev_reduced_6.Score.apply(lambda x: x-1)\n",
    "# amazon_rev_reduced[amazon_rev_reduced.Score == 0]\n",
    "\n",
    "rev_train_6, rev_test_6 = train_test_split(amazon_rev_reduced_6, train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of number of words per review in the train dataset')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words_per_review_6 = rev_train_6.Text.apply(lambda x: len(x.split(\" \")))\n",
    "words_per_review_6.hist(bins = 100)\n",
    "\n",
    "plt.title(\"Histogram of number of words per review in the train dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the train dataset, the 80th quantile of the number of words per review is 131.0\n"
     ]
    }
   ],
   "source": [
    "print(\"In the train dataset, the 80th quantile of the number of words per review is\",words_per_review_6.quantile(0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 112964 different words in the original corpus\n",
      "The 80th percentile is:  90371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\envs\\extended_case_7\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33973 different words in the corpus after using only lowercase and removing special characters\n",
      "The 80th percentile is: 27178\n"
     ]
    }
   ],
   "source": [
    "words_orig_6 = amazon_rev_reduced_6.Text.apply(lambda x: x.split(\" \"))\n",
    "words_orig_6 = [word for sublist in words_orig_6 for word in sublist]\n",
    "print(f\"There are {len(set(words_orig_6))} different words in the original corpus\")\n",
    "print(f\"The 80th percentile is: \", int(len(set(words_orig_6)) * 0.8))\n",
    "\n",
    "amazon_rev_reduced_6[\"text_new\"] = amazon_rev_reduced_6.Text.apply(lambda x: x.lower())\n",
    "\n",
    "special_chars = amazon_rev_reduced_6.text_new.apply(lambda x: [each for each in list(x) if not each.isalnum() and each != ' '])\n",
    "flat_list = [item for sublist in special_chars for item in sublist]\n",
    "# print(set(flat_list))\n",
    "\n",
    "amazon_rev_reduced_6.text_new = amazon_rev_reduced_6.text_new.apply(\n",
    "    lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "words_prep_6 = amazon_rev_reduced_6.text_new.apply(lambda x: x.split(\" \"))\n",
    "words_prep_6 = [word.strip() for words in words_prep_6 for word in words]\n",
    "print(f\"There are {len(set(words_prep_6))} different words in the corpus after using only lowercase and removing special characters\")\n",
    "print(f\"The 80th percentile is: {int(len(set(words_prep_6)) * 0.8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_6 = tokenizer.texts_to_sequences(rev_train_6.Text)\n",
    "test_seq_6 = tokenizer.texts_to_sequences(rev_test_6.Text)\n",
    "train_sequences_6 = pad_sequences(\n",
    "    train_seq_6, maxlen=116, dtype='int32', padding='pre', truncating='pre',\n",
    "    value=0.0)\n",
    "test_sequences_6 = pad_sequences(\n",
    "    test_seq_6, maxlen=116, dtype='int32', padding='pre', truncating='pre',\n",
    "    value=0.0)\n",
    "\n",
    "train_labels_6 = rev_train_6.Score\n",
    "test_labels_6 = rev_test_6.Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "19200/19200 [==============================] - 26s 1ms/sample - loss: 1.3609 - accuracy: 0.4061 - val_loss: 1.2311 - val_accuracy: 0.4762\n",
      "Epoch 2/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 1.0945 - accuracy: 0.5424 - val_loss: 1.2072 - val_accuracy: 0.5004\n",
      "Epoch 3/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.9295 - accuracy: 0.6261 - val_loss: 1.2006 - val_accuracy: 0.5123\n",
      "Epoch 4/10\n",
      "19200/19200 [==============================] - 24s 1ms/sample - loss: 0.7751 - accuracy: 0.7027 - val_loss: 1.2415 - val_accuracy: 0.5231\n",
      "Epoch 5/10\n",
      "19200/19200 [==============================] - 24s 1ms/sample - loss: 0.6430 - accuracy: 0.7606 - val_loss: 1.3667 - val_accuracy: 0.5235\n",
      "Epoch 6/10\n",
      "19200/19200 [==============================] - 24s 1ms/sample - loss: 0.5178 - accuracy: 0.8147 - val_loss: 1.4809 - val_accuracy: 0.5298\n",
      "Epoch 7/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.4120 - accuracy: 0.8560 - val_loss: 1.6632 - val_accuracy: 0.5342\n",
      "Epoch 8/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.3214 - accuracy: 0.8929 - val_loss: 1.8150 - val_accuracy: 0.5221\n",
      "Epoch 9/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.2419 - accuracy: 0.9230 - val_loss: 2.0219 - val_accuracy: 0.5102\n",
      "Epoch 10/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.1774 - accuracy: 0.9463 - val_loss: 2.2184 - val_accuracy: 0.5152\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_6_0 = model.fit(train_sequences_6, train_labels_6, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 1.3680 - accuracy: 0.4022 - val_loss: 1.2371 - val_accuracy: 0.4702\n",
      "Epoch 2/10\n",
      "19200/19200 [==============================] - 37s 2ms/sample - loss: 1.1065 - accuracy: 0.5349 - val_loss: 1.2027 - val_accuracy: 0.4975\n",
      "Epoch 3/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.9504 - accuracy: 0.6152 - val_loss: 1.1915 - val_accuracy: 0.5158\n",
      "Epoch 4/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.7989 - accuracy: 0.6934 - val_loss: 1.2387 - val_accuracy: 0.5294\n",
      "Epoch 5/10\n",
      "19200/19200 [==============================] - 39s 2ms/sample - loss: 0.6709 - accuracy: 0.7479 - val_loss: 1.3353 - val_accuracy: 0.5200\n",
      "Epoch 6/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.5527 - accuracy: 0.7954 - val_loss: 1.4597 - val_accuracy: 0.5250\n",
      "Epoch 7/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.4376 - accuracy: 0.8466 - val_loss: 1.6683 - val_accuracy: 0.5244\n",
      "Epoch 8/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.3541 - accuracy: 0.8779 - val_loss: 1.7800 - val_accuracy: 0.5127\n",
      "Epoch 9/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.2724 - accuracy: 0.9083 - val_loss: 1.9705 - val_accuracy: 0.5092\n",
      "Epoch 10/10\n",
      "19200/19200 [==============================] - 38s 2ms/sample - loss: 0.2014 - accuracy: 0.9374 - val_loss: 2.1569 - val_accuracy: 0.5046\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(30000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_6_1 = model.fit(train_sequences_6, train_labels_6, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "In the first model, maintaining only 20.000 most frequent words and still truncating the sequences at position 116, we achieve a different result of the model compared to the original. The accuracy of the validation was around 53.4% in the 7th epoch. After that, it seemed that the model started overfitting again.\n",
    "\n",
    "Nevertheless, another try was done with the 30.000 most frequent words since, as explored above, we see that for this larger dataset the quantiles changed considerably (even for the 80th quantile of the length of the sequence, as it increase by 10 compared to the original). This last model also achieved an accuracy of around 53%, this time in the 4th epoch. The numbers for this model were selected due to computational limitations but would be interesting to check for higher amount of words and sequence lengths.\n",
    "\n",
    "Therefore, it seems that the model found a new ceiling in accuracy but still overfits. **In fact, the validation loss keeps reaching a minimum in the models around the 3rd epoch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization through early stopping\n",
    "\n",
    "We have consistently seen that our neural network overfits at around the third epoch. Hence, another form of regularization is to end training early if validation loss starts increasing. (This is similar to the validation curves we used when constructing classification models.) Although the network will not have found an optimal function in the training data, the looser function that it has found will likely be more generalizable.\n",
    "\n",
    "You can do this manually by inspecting the data as we have done above and modifying the `epochs` argument in `fit()`, but Keras also allows you to easily do this automatically via an [`EarlyStopping` callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Experiment with the `EarlyStopping` callback and explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 1.3610 - accuracy: 0.4069 - val_loss: 1.2214 - val_accuracy: 0.4746\n",
      "Epoch 2/10\n",
      "19200/19200 [==============================] - 24s 1ms/sample - loss: 1.1074 - accuracy: 0.5378 - val_loss: 1.2342 - val_accuracy: 0.4940\n",
      "Epoch 3/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.9472 - accuracy: 0.6185 - val_loss: 1.1876 - val_accuracy: 0.5123\n",
      "Epoch 4/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.8016 - accuracy: 0.6901 - val_loss: 1.2224 - val_accuracy: 0.5246\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', min_delta=0.5, patience=3, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_7_0 = model.fit(train_sequences_6, train_labels_6, validation_split=0.2, epochs=10, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "19200/19200 [==============================] - 27s 1ms/sample - loss: 1.3739 - accuracy: 0.3996 - val_loss: 1.2291 - val_accuracy: 0.4698\n",
      "Epoch 2/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 1.1049 - accuracy: 0.5435 - val_loss: 1.2093 - val_accuracy: 0.5006\n",
      "Epoch 3/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.9380 - accuracy: 0.6265 - val_loss: 1.2169 - val_accuracy: 0.5088\n",
      "Epoch 4/10\n",
      "19200/19200 [==============================] - 25s 1ms/sample - loss: 0.7886 - accuracy: 0.6967 - val_loss: 1.2502 - val_accuracy: 0.5277\n",
      "Epoch 5/10\n",
      "19200/19200 [==============================] - 26s 1ms/sample - loss: 0.6534 - accuracy: 0.7569 - val_loss: 1.3371 - val_accuracy: 0.5321\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor='val_loss', min_delta=0.5, patience=4, verbose=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history_7_1 = model.fit(train_sequences_6, train_labels_6, validation_split=0.2, epochs=10, callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "From the models, we had already seen that the best validation accuracy was found one or two epochs after the loss validation reached its minimum and started increasing again. Therefore, i used this particularity of the model to tune the early stopping. \n",
    "\n",
    "Moreover, it is seen that the val_loss is not monotonically decreasing, as it can increase a bit before reaching its minimum and then start increasing constantly after that. Therefore, it was necesary to add a higher \"patience\" parameter to the early_stopping.\n",
    "\n",
    "We can see that after tuning a bit the early_stopping, we end up with the model having a validation accuracy as high as was expected for the best model, around 53%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in most previous cases, we used *three* splits of our data instead of two. All of our model tuning has been done on the validation set, and we have not even touched the test set that we split off right at the start.\n",
    "\n",
    "For experiments, it's very important that your model is only run **once** on your test set. As there is so much randomness at play, it's vital to not \"cherry-pick\" the best results, so optimize as much as you want on the validation set, but keep the test set until the end and all official results should be based on the single run of the test set (or whatever configuration was decided *before the experiment started*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "Let's take the model configuration that resulted in the highest validation accuracy and use that one as our final model. Evaluate this configuration on how well it performs on the test set, and furthermore diagnose *what kinds of mistakes it makes*. Explain whether these mistakes are expected or not, and print some of these poorly classified reviews. Given the mistakes the model made, how would you then go back and try to improve the model or optimize the tuning steps?\n",
    "\n",
    "**Hint:** You can use the [`predict_classes`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_classes) method on your model to get the most probable class directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 1s 105us/sample\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict_classes(test_sequences_6,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set was:  0.5176666666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy on the test set was: \", np.array([predicted == test_labels_6]).sum()/len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five really bad reviews (difference of more than 2 in predicted score)\n",
      "\n",
      "Review:  I just bought the Zebra flavor after sampling at a holiday booth. The sample had chocolate drizzled on each piece.  False representation. The actual bag contains few pieces with chocolate. (less than 10& of pieces have chocolate) It looks and tastes like Crunch 'n Munch or Cracker Jacks.  If you're looking for gourmet popcorn, this is not it.\n",
      "The predicted score was:  4\n",
      "The real score was:  1\n",
      "\n",
      "--------------\n",
      "\n",
      "Review:  My dog seems to really like them, but the sweet potato gets very, very hard once the bag is opened.  My concern is they are about and inch and half long and do not want to have my dog swallowing something that size and  hardness.  As canines are gulpers, I am concerned.  The same product with the the chicken and apple is much softer.\n",
      "The predicted score was:  3\n",
      "The real score was:  1\n",
      "\n",
      "--------------\n",
      "\n",
      "Review:  This extract has a very strong alcohol aroma, so much so that it is difficult to discern any pistachio flavor. You might as well use a liquer instead- you'd probably get a better flavor.\n",
      "The predicted score was:  5\n",
      "The real score was:  2\n",
      "\n",
      "--------------\n",
      "\n",
      "Review:  This product description does not say whether this product is pasteurized. Alton Brown on Good Eats says that if pomegranate juice is pasteurized you lose a lot of the antioxidant benefits. I would likely change to more stars if it is not pasteurized.\n",
      "The predicted score was:  4\n",
      "The real score was:  1\n",
      "\n",
      "--------------\n",
      "\n",
      "Review:  So, Amazon, you're back to selling whale meat, fois gras, and other gross foods?  What gives?  Is it because few American's eat it, or even know what it is?  And, it's expensive which helps your bottom line.  So, why not take stuff American's don't or are forbidden to eat (remember the ban on whale slaughter is now world wide and it makes your selling of it illlegal).  People are becoming more informed on a daily basis, and finding this kind of stuff on Amazon is little less than appalling, much less, law disregarding.\n",
      "The predicted score was:  4\n",
      "The real score was:  1\n",
      "\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted == test_labels_6\n",
    "i=0\n",
    "counter = 0\n",
    "\n",
    "print(\"First five really bad reviews (difference of more than 2 in predicted score)\\n\")\n",
    "\n",
    "while counter < 5:\n",
    "    diff = predicted[i] - test_labels_6.values[i]\n",
    "    if  diff >= 2:\n",
    "        print(\"Review: \", rev_test_6.Text.values[i])\n",
    "        print(\"The predicted score was: \", predicted[i]+1)\n",
    "        print(\"The real score was: \", test_labels_6.values[i]+1)\n",
    "        print(\"\\n--------------\\n\")\n",
    "        \n",
    "        counter += 1\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "\n",
    "We can see that the model has an accuracy of about 51.7%. This represented 3016 samples of the 6000 in the test set. This is consistent with the validation accuracy that was obtained during the training.\n",
    "\n",
    "When looking at badly classified reviews, meaning that the difference between classification and real score was more than 2. We can see that these reviews tend to be gentle, in the sense that they can be sarcastic, using generally positive or neutral adjectives to describe what is not a good feeling or result.\n",
    "\n",
    "We can see this in: \"The sample had chocolate drizzled on each piece\" and it is refering to the fact that the products creates false expectations.\n",
    "\n",
    "\"My dog seems to really like them\" but then speaks of characteristics of the product that imply a negative experience.\n",
    "\n",
    "\"This extract has a very strong alcohol aroma\", \"I would likely change to more stars\". The last review is a curious case because it is not direct for me to notice what the model is predicting.\n",
    "\n",
    "Nevertheless, we see that these reviews have \"difficult\", \"not\", \"but\" and these words might be connected to negative experiences but they can also appear in positive reviews as well.\n",
    "\n",
    "**It would be interesting to try the model including the most common 2- and 3-grams as well as making sure to remove the stopwords that might be polluting the dataset used to train the model.**\n",
    "\n",
    "Also, as mentioned before, using longer sequences and more data in the model embedder could be positive, as we saw that including 6000 samples of each score resulted in a much higher number of unique words and length quantiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
